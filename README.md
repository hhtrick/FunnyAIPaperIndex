# ğŸ¤ª FunnyAIPaperIndex | æœ‰è¶£çš„ AI è®ºæ–‡æ ‡é¢˜é›†

> **"Research doesn't have to be boring."**

## ğŸ“– About / å…³äºæœ¬é¡¹ç›®

**[English]**
I am an ordinary graduate student, buried in piles of literature every day, often finding the work dull and tedious. However, stumbling upon creative and humorous paper titles never fails to bring a smile to my face. 

These titles remind me that the authors behind these complex academic texts are "flesh-and-blood humans" with their own lives, hobbies, and sense of humor. Whenever I encounter such titles, the boredom of reading literature fades away. I created this repository to collect these moments of joy. I will update it periodically and welcome everyone to contribute!

**[ä¸­æ–‡]**
æˆ‘æ˜¯ä¸€åæ™®é€šçš„ç§‘ç ”ç ”ç©¶ç”Ÿï¼Œæ—¥å¤ä¸€æ—¥åœ°åŸ‹é¦–äºæµ©å¦‚çƒŸæµ·çš„æ–‡çŒ®ä¸­ï¼Œæ—¶å¸¸æ„Ÿåˆ°æ¯ç‡¥ä¸ä¹å‘³ã€‚ç„¶è€Œï¼Œæ¯å½“å¶ç„¶ç¥è§é‚£äº›å……æ»¡åˆ›æ„çš„è®ºæ–‡æ ‡é¢˜ï¼Œæ€»èƒ½è®©æˆ‘ä¼šå¿ƒä¸€ç¬‘ã€‚

è¿™äº›æœ‰è¶£çš„æ ‡é¢˜è®©æˆ‘çœŸåˆ‡åœ°æ„Ÿå—åˆ°ï¼Œé‚£äº›å­¦æœ¯æ–‡å­—èƒŒåçš„ä½œè€…ï¼Œä¹Ÿæ˜¯æœ‰è¡€æœ‰è‚‰çš„äººï¼Œæ‹¥æœ‰ç€é²œæ´»çš„ç”Ÿæ´»ä¸æœ‰è¶£çš„çµé­‚ã€‚æ¯æ¯è¯»åˆ°è¿™æ ·çš„æ ‡é¢˜ï¼Œé˜…è¯»æ–‡çŒ®å¸¦æ¥çš„ç–²æƒ«ä¼¼ä¹ä¹Ÿå°±æ¶ˆè§£äº†å¤§åŠã€‚æˆ‘ä¼šä¸å®šæœŸæ›´æ–°è¿™ä¸ªåˆ—è¡¨ï¼Œè®°å½•é‚£äº›è®©æˆ‘çœ¼å‰ä¸€äº®çš„ç¬é—´ï¼Œä¹Ÿçƒ­çƒˆæ¬¢è¿å¤§å®¶æäº¤ PR è¡¥å……ï¼

---

## ğŸ§¾ The Collection / è®ºæ–‡æ”¶å½•

### ğŸ¬ Pop Culture References (æµè¡Œæ–‡åŒ–è‡´æ•¬)

| Title (Paper Link) | Authors | Fun Factor / æ¢— |
| :--- | :--- | :--- |
| **[May the Force be with You: Unified Force-Centric Pre-Training for 3D Molecular Conformations](http://arxiv.org/abs/2308.14759)** | Rui Feng, Qi Zhu, et al. | **Star Wars (æ˜Ÿçƒå¤§æˆ˜)** <br> The title quotes the Jedi phrase directly. Ideally suited for a paper about "Force-centric" training.<br>ç›´æ¥å¥—ç”¨æ˜Ÿæˆ˜çš„åè¨€ï¼Œå¥‘åˆâ€œåŠ›åœºâ€ç›¸å…³çš„ç ”ç©¶ã€‚ |
| **[BatmanNet: Bi-branch masked graph transformer autoencoder for molecular representation](https://doi.org/10.1093/bib/bbad400)** | Zhen Wang, Zheng Feng, et al. | **Batman (è™è ä¾ )** <br> The name is a "so perfect" acronym: **B**i-branch **A**symmetric **T**ransformer **M**asked **A**utoencoder **N**etwork. It uses a "Masking" strategy (60% mask ratio), symbolically fitting the Dark Knight.<br>åå­—ä¸ä»…æ˜¯â€œå®Œç¾â€çš„ç¼©å†™ï¼ˆ**B**i-branch **A**symmetric **T**ransformer **M**asked **A**utoencoder **N**etï¼‰ï¼Œè€Œä¸”æ ¸å¿ƒç®—æ³•ä¾èµ–â€œæ©ç ï¼ˆMaskingï¼‰â€æœºåˆ¶ï¼Œä»åå­—åˆ°å†…æ ¸éƒ½è‡´æ•¬äº†è€çˆ·ã€‚ |
| **[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)** | Jacob Devlin, et al. | **Sesame Street (èŠéº»è¡—)** <br> Followed the trend started by ELMo, solidifying the "Muppet" naming convention in NLP.<br>ç´§éš ELMo ä¹‹åï¼Œç¡®ç«‹äº† NLP é¢†åŸŸç”¨èŠéº»è¡—è§’è‰²å‘½åçš„ä¼ ç»Ÿã€‚ |
| **[Deep Contextualized Word Representations (ELMo)](https://arxiv.org/abs/1802.05365)** | Matthew E. Peters, et al. | **Sesame Street (èŠéº»è¡—)** <br> One of the OGs of funny acronyms. ELMo stands for "Embeddings from Language Models".<br>æœ‰è¶£ç¼©å†™é¼»ç¥–ï¼Œç®—æ˜¯å¼€åˆ›äº†èŠéº»è¡—æ—¶ä»£ã€‚ |
| **[ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223)** | Yu Sun et al. (Baidu) | **Ernie (è‰¾å°¼)** <br> BERT's best friend in Sesame Street. The authors worked really hard to make the acronym fit.<br>BERT çš„åŸºå‹è‰¾å°¼ã€‚ä¸ºäº†å‡‘ç¼©å†™å·²ç»ä¸æ‹©æ‰‹æ®µäº†ğŸ˜‚ã€‚ |
| **[Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)** | Manzil Zaheer et al. (Google) | **Big Bird (å¤§é¸Ÿ)** <br> Designed for "Longer" sequences, so naturally named after the tall yellow bird.<br>ä¸“é—¨å¤„ç†é•¿åºåˆ—çš„æ¨¡å‹ï¼Œæ‰€ä»¥ç”¨èŠéº»è¡—é‡Œæœ€é«˜çš„å¤§é¸Ÿå‘½åï¼Œææ²è€å¸ˆçš„è¯„ä»·æ˜¯â€”â€”å·²ç»å®Œå…¨æ˜¯ä¸ºäº†å‡‘è¿™ä¸ªåå­—äº†ã€‚ |


---

## ğŸ¤ Contribution / å¦‚ä½•è´¡çŒ®

Found a funny paper? Make a Pull Request!
å‘ç°äº†æœ‰è¶£çš„è®ºæ–‡ï¼Ÿæ¬¢è¿æäº¤ PRï¼

1. Fork this repository.
2. Add the paper to the list following the format above.
3. Submit a Pull Request.

Let's make research a little bit more fun! âœ¨
è®©æˆ‘ä»¬æŠŠç§‘ç ”å˜å¾—æ›´æœ‰è¶£ä¸€ç‚¹ç‚¹ï¼
